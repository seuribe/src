{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7d033a-595c-4c3f-8734-c61d7dd96c10",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Basics\n",
    "\n",
    "## Part 1: Introduction\n",
    "\n",
    "### What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a *Paradigm* (or technique) in Machine Learning, that allows us to train an *Agent* to complete a task without providing detailed explanations or previous examples of how the task is supposed to be completed. In order to learn, the Agent will attempt to solve the task multiple times, trying out different *Actions* in a given *Environment* and paying attention to what happens afterwards. As the agent *Explores* the environment and the consequences of its actions, it builds up a *Policy* that will guide its actions in the future.\n",
    "\n",
    "In simpler terms, with RL we can learn to solve a problem by dividing it in sub-problems (or sub-tasks, or steps), trying many times different actions for each step, and remembering how well or bad each one worked. At its core, the learning takes the form of a \"memory\" where we remember the results of each action at any given step, and which we update with our new experiences. When we think that we have learned enough, we can stop updating this memory and make use of the experience, in order to solve the task effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab69021-aa2b-415d-967a-f971e32c065b",
   "metadata": {},
   "source": [
    "### Finding the exit\n",
    "\n",
    "We will learn how Reinforcement Learning works by solving a simple problem: finding the exit to a maze. Our maze is an 8 by 8 grid of cells, each containing either a wall, or empty space. In addition we can see our starting position (top left corner) and the exit.\n",
    "\n",
    "![](maze.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461644d-2741-4154-b05f-8fdeeff9a3b1",
   "metadata": {},
   "source": [
    "### How do we solve a maze?\n",
    "\n",
    "With a maze as simple as the one above, it seems straightforward: just start walking until you reach the exit. But if we want to program an algorithm that can find the exit, we need to describe our actions in more precise terms. We first need a start and end positions, which are already given above. Then, we need to describe with some level of detail our actions. In this case, when standing on a cell, our possible actions are to move in any of the four directions (up, down, left, right) as far as they are empty cells, that is, they don't have a wall. On top of that, we want to exit the maze as fast as possible, that is, we want to find the shortest path between the start and end positions. By the way, we don't know if we've reached the end position until we are actually there, which means that you cannot \"look ahead\" of you and just sprint for the exit, you first need to actually step on that cell to know that you're there.\n",
    "\n",
    "There are several methods that can solve this problem, some of which might be easier to implement than Reinforcement Learning for such a basic problem. But as we will see soon, the power of RL resides in that the core algorithms can be used, with few changes, to solve much more complex problemes which other methods cannot. So, how do we use RL to exit to the maze?\n",
    "\n",
    "### Knowing if we are on the right path\n",
    "\n",
    "Imagine that you are visiting a friend, who lives in a part of a city that you don't know very well. As you stand at a bus stop, you try to remember in which direction you should go. You start walking, and cannot recognize any building in that street, so you go back to the bus stop, and try another direction. You stop each corner and evaluate each new possible direction based on what you remember from the previous times you've been there. As you recognize (or not) some houses and streets, you choose your way until you finally arrive at the house. Reinforcement learning works in a very similar way.\n",
    "\n",
    "### How Agents learn through rewards\n",
    "\n",
    "At the core of this learning is the idea of learning through rewards. Every time you choose a direction and took a look at the street and its houses, you were \"rewarded\" with the memory of the place, which \"reinforced\" the idea that you are on the right track. Although the analogy is not perfect, it gives us a good idea of how RL works.\n",
    "\n",
    "Like you on the unknown neighborhood, the agent looking for the exit will be rewarded every time it makes a decision, and will use these rewards as a \"memory\" of the maze. Because we are working with computers, rewards will be represented by a numeric value. But this memory starts empty, and needs to be filled in. The first time you visit your friend, you really don't know which way to take from the bus stop (let's imagine there are no Maps Apps), and each street will look equally unfamiliar. Your reward will therefore be the same for every street the first time you visit it. Likewise, the first time an agent attempts to solve the maze, it was no previous memory of which choices to make. As it takes off from it's initial position (0,0), it could go either to its right (1,0) or down (0,1). When we implement the system, we could be tempted to return a better reward for going down, because a simple glance at the maze tells us that going to the right is a dead end. But that would be cheating, becuase we are basing our rewards on knowing the solution beforehand (if we do, we don't need any algorithm for solving the maze!) When we use RL, we cannot do that. In this case, each step is, without any knowledge of the solution, equivalent, and both should reward the agent equally.\n",
    "\n",
    "### Episodic Learning\n",
    "\n",
    "You might not remember the precise way to your friend's house after just one visit, and it might take you several times to be able to choose your path without any mistake. The same happens to our agent trying to exit the maze: it will not only need many attempts before it finds the way out, but also before it remembers it so well that it can solve the maze without a single mistake. In Reinforcement Learning terms, each attempt is called an *episode*.\n",
    "\n",
    "### Delayed rewards\n",
    "\n",
    "But if we know nothing of the maze at the beginning, and the reward we get from all empty cells is the same, how can we learn which is the way to the exit? Because there is one special case, in which the reward changes: when we reach the exit. We said above, that the agent only knows where the exit is when it has reached it. In reality, the agent doesn't \"know\" that the exit \"state\" is different from others, but it will receive a different reward. In this case, a proper \"reward\", which means a high numeric value.\n",
    "\n",
    "Let's go back to our analogy with finding your friend's house. If you have already been there more than once, then you can probably remember how the street looks like. So when you arrive at the corner, you know that you are (almost there). And can probably tell from two streets away already. But the further away that you are, the less clear that it might be how close you are. In RL we find a similar situation. Imagine that the agent has already found the exit in a previous episode, and it received a good reward. Now it finds itself in the cell just left to it:\n",
    "\n",
    "![](maze_next_to_exit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb82b80-abce-429e-8996-1827977905d6",
   "metadata": {},
   "source": [
    "From that state, the agent might remember that walking right gives it a very high reward, while the reward for walking down isn't that interesting. An Agent that wants to maximize its rewards will move to the right, and reach the exit.\n",
    "\n",
    "### Maximizing Rewards\n",
    "\n",
    "The reason why we mention maximizing rewards, is because that's the way in which we \"nudge\" our agent into the right direction: by asking them to maximize the rewards that they accumulate at the end of a solving attempt / episode. What does that mean? That for each state, the agent calculates the expected cumulative reward that they will receive and uses that information to decide which state is most desiderable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bbf5c-111b-4f6b-a832-9e7d9fcc4dac",
   "metadata": {},
   "source": [
    "### Training vs solving: explore vs. exploit\n",
    "\n",
    "In the above example we show the values that an agent gives to each possible state (position in the maze) after being trained. How are agents trained? And how is training different from solving the problem? One of the core concepts of Reinforcement Learning, and something that makes it different from other machine learning methods, is the idea of explore vs. exploit. \n",
    "\n",
    "If we are asked to solve a problem we know nothing about, and we are not given any instruction or guidance, then we have no option but to try out things (explore). Once we feel confident that we have learned enough, we can apply that knowledge to solve the problems (explot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ff595-76a2-4f6d-ae2d-f7f64c44fb44",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "So far we have covered the main aspects of Reinforcement Learning in an intuitive way. Let's recap the key concepts:\n",
    "\n",
    "- The ***Agent*** is the entity that we are training. It can represent a concrete entity from the real world (for example, a Robot) or something abstract (a player in a Go game).\n",
    "\n",
    "- The ***Environment*** is where the Agent does its learning. Although it usually remains unchanged, it can also be affected by the actions of the Agent, or even behave in a way that is not deterministic. The Agent will have to learn to deal with this.\n",
    "\n",
    "- While the Agent explores the Environment, it will need to somehow remember where it is, that is, its ***State***.\n",
    "\n",
    "- The objective of the Agent is to learn how to complete a Task. Tasks are composed of multiple ***Actions***. Whenever the Agent executes an Action, it will most probably change its State.\n",
    "\n",
    "- In a given state, there might be multiple possible actions. In order to learn the best possible action for each state, the Agent is given a ***Reward*** every time it executes an action.\n",
    "\n",
    "- As the explores states, executes actions and receives rewards, it will store all this information inside a ***Policy***.\n",
    "\n",
    "- But the Agent doesn't try only once to complete the task, it will try multiple times. Each attempt is called an ***Episode***\n",
    "\n",
    "- As the Agent starts learning, it will not know much about the environment and its rewards. But as the Policy accumulates more information, it will be important to decide where the Agent continues ***Exporing***, or starts  ***Exploiting*** the learnings it has so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851c8bf-024b-42ad-b826-744f54857218",
   "metadata": {},
   "source": [
    "## Part 2: Implementation\n",
    "\n",
    "We will first create the Python classes that will represent our core concepts. These classes do very little on their own, and need to be extended to solve a particular problem, like solving the maze.\n",
    "\n",
    "### Actions, States and the Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b70012-7ff3-4103-acfa-ae384322ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e541c1f-2c06-4ddb-b0e4-5ecfa724ff39",
   "metadata": {},
   "source": [
    "These classes don't do anything right now because they depend on the problem that we want to solve. In the case of the Maze, Actions are the directions in which the agent moves, and the State is its current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cb7ef7-4145-4af0-819e-f1e6705f7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class Environment:\n",
    "    state:State\n",
    "\n",
    "    def __init__(self, state:State):\n",
    "        self.state = state\n",
    "\n",
    "    def isEndState(self) -> bool:\n",
    "        return self.isWinState() or self.isTieState() or self.isLoseState()\n",
    "\n",
    "    def isWinState(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def isLoseState(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def isTieState(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def execute(self, action:Action) -> int:\n",
    "        self.state = self.getNewState(action)\n",
    "        return self.getReward()\n",
    "\n",
    "    def getNewState(self, action:Action) -> State:\n",
    "        return self.state\n",
    "\n",
    "    def getReward(self):\n",
    "        return 100 if self.isWinState() else -1\n",
    "\n",
    "    def getState(self) -> State:\n",
    "        return self.state\n",
    "\n",
    "    def getAllPossibleActions(self) -> List[Action]:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d2657-acce-4998-8381-b4065d2c5d75",
   "metadata": {},
   "source": [
    "The ```Environment``` class also doesn't do much by itself, but it will make it easier to specify our problem and decouple (separate) the core RL classes from the problem-dependent ones. It contains the current state of the agent, as well as methods that tell the current state of the agent (win, lose, tie). The ```execute``` method changes the state of the agent depending on the ```Action``` and returns a reward. The method ```getAllPossibleActions``` must be properly implemented by a class inheriting from ```Environment``` and should return all possible actions given the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98830151-731f-4e0a-9ef4-c0b23704dd1f",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The following two classes implement the Q-Learning algorithm. This belongs to the so called \"Temporal-Difference\" (TD) family of learning methods, which work by training a *Policy* that can find the best action for any given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beacedc5-8f95-4b27-9bf4-d7c8ee819f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QMemory:\n",
    "    class ActionRewards:\n",
    "        maxAction:Action\n",
    "        def __init__(self):\n",
    "            self.actions = {}\n",
    "            self.maxAction = None\n",
    "\n",
    "        def getExpectedReward(self, action:Action):\n",
    "            return self.actions.get(action, 0)\n",
    "\n",
    "        def getMaxReward(self):\n",
    "            return 0 if self.maxAction is None else self.actions[self.maxAction]\n",
    "\n",
    "        def setExpectedReward(self, action:Action, reward):\n",
    "            self.actions[action] = reward\n",
    "            if not self.maxAction or reward > self.actions[self.maxAction]:\n",
    "                self.maxAction = action\n",
    "\n",
    "    sar: Dict[State, ActionRewards]\n",
    "\n",
    "    def __init__(self, learningRate = 0.9, discountRate = 0.5):\n",
    "        self.learningRate = learningRate\n",
    "        self.discountRate = discountRate\n",
    "        self.sar = {}\n",
    "\n",
    "    def getBestAction(self, state:State) -> Action:\n",
    "        return self.getActionRewards(state).maxAction\n",
    "\n",
    "    def getActionRewards(self, state:State) -> ActionRewards:\n",
    "        if state in self.sar:\n",
    "            return self.sar[state]\n",
    "\n",
    "        ars = self.ActionRewards()\n",
    "        self.sar[state] = ars\n",
    "\n",
    "        return ars\n",
    "\n",
    "    def getMaxReward(self, state:State):\n",
    "        return self.getActionRewards(state).getMaxReward()\n",
    "\n",
    "    def update(self, oldState:State, action:Action, newState:State, reward):\n",
    "        oldSA = self.getActionRewards(oldState)\n",
    "        currentValue = oldSA.getExpectedReward(action)\n",
    "        newSA = self.getActionRewards(newState)\n",
    "        maxExpectedNew = newSA.getMaxReward()\n",
    "        newExpectedReward = currentValue + self.learningRate * (reward + self.discountRate * maxExpectedNew - currentValue)\n",
    "        oldSA.setExpectedReward(action, newExpectedReward)\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    qMemory:QMemory\n",
    "\n",
    "    def __init__(self, learningRate = 0.9, discountRate = 0.5):\n",
    "        self.qMemory = QMemory(learningRate, discountRate)\n",
    "\n",
    "    def pickAction(self, env:Environment, exploitRate = 1) -> Action:\n",
    "        if random.random() <= exploitRate:\n",
    "            st = env.getState()\n",
    "            bestAction = self.qMemory.getBestAction(st)\n",
    "            if bestAction:\n",
    "                return bestAction\n",
    "\n",
    "        return self.randomAction(env)\n",
    "\n",
    "    def randomAction(self, env:Environment) -> Action:\n",
    "        actions = list(env.getAllPossibleActions())\n",
    "        return random.choice(actions) if actions else None\n",
    "\n",
    "    def update(self, oldState:State, action:Action, newState:State, reward):\n",
    "        self.qMemory.update(oldState, action, newState, reward)\n",
    "\n",
    "    def getMaxReward(self, state):\n",
    "        return self.qMemory.getMaxReward(state)\n",
    "\n",
    "    def numKnownStates(self):\n",
    "        return len(self.qMemory.sar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b55ce-e842-4bb6-86e4-e3830d88fc26",
   "metadata": {},
   "source": [
    "As you can see, the policy contains a \"Memory\" ```QMemory``` which it consults for deciding which action to take in the ```pickAction``` method. This memory is used to predict the so called *Q function* and its values are is updated in the ```update``` method by using the following formula.\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\longleftarrow Q(S_t, A_t) + \\alpha [ R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) ]\n",
    "$$\n",
    "\n",
    "The Q function tells us roughly what we should expect if we take action *A* when in state *S*. The objective of Q-learning is, as it names implies, to \"learn\" the values of the Q function. The values of the function are updated at each step using:\n",
    "* The current state ($S_t$). The subscript *t* is meant to show which state do we refer to within a sequence of states.\n",
    "* The action taken ($A_t$)\n",
    "* The new state ($S_{t+1}$) to which we transition after taking action ($A_t$) from state ($S_t$). Therefore the index *t+1*, meant to show us that it is the next state.\n",
    "* The reward ($R_t$) the agent receives as a result of taking that action.\n",
    "\n",
    "Additionally, there are two important parameters:\n",
    "* $\\alpha$ is the \"learning rate\". As you can see in the formula, it influences how much the state will be modified by the new reward and the expected maximum reward at the new state.\n",
    "* $\\gamma$ is the \"discount rate\", which controls how much the expected future rewards (from the state $S_{t+1}$ onwards) affect the Q value for the current state.\n",
    "\n",
    "> #### Excercise: What happens if the value of the learning rate is 0? And if it is 1?\n",
    "> #### Excercise: What happens if the discount rate is very low or very high?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f2cc1-28a3-43c3-96c7-0e202a30a54a",
   "metadata": {},
   "source": [
    "### Implementing the maze\n",
    "\n",
    "Now that we have laid out the foundation of Q-Learning and the core classes that implement it, we can continue with the implementation of the Maze problem.\n",
    "\n",
    "First, some helper classes that will it easier to represent the grid, agent and state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c25998-9153-4832-9e4a-016932598c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Direction(Enum):\n",
    "    \"\"\" Simple representation of a direction for movement in the grid \"\"\"\n",
    "    Up = (0, -1)\n",
    "    Down = (0, 1)\n",
    "    Left = (-1, 0)\n",
    "    Right = (1, 0)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.value[0], self.value[1]))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{(self.value[0], self.value[1])}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\" A position in the grid. x is the column and y the row \"\"\"\n",
    "    x:int\n",
    "    y:int\n",
    "\n",
    "    def move(self, dir:Direction):\n",
    "        \"\"\" returns a new position moved in the direction \"\"\"\n",
    "        return Position(self.x + dir.value[0], self.y + dir.value[1])\n",
    "\n",
    "    def allMoves(self):\n",
    "        poss = []\n",
    "        for dir in Direction:\n",
    "            poss.append(self.move(dir))\n",
    "        return poss\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.x, self.y))\n",
    "\n",
    "class Grid:\n",
    "    \"\"\" The maze is represented by this grid, itself containing a two-dimensional array of ints \"\"\"\n",
    "    EMPTY = 0\n",
    "    WALL = 1\n",
    "    START = 2\n",
    "    END = 3\n",
    "\n",
    "    gridData:List[List[int]]\n",
    "    def __init__(self, data):\n",
    "        self.gridData = deepcopy(data)\n",
    "\n",
    "    def isWall(self, pos:Position):\n",
    "        \"\"\" is there a wall a this position? \"\"\"\n",
    "        return self.gridData[pos.y][pos.x] is self.WALL\n",
    "\n",
    "    def get(self, pos:Position):\n",
    "        \"\"\" what is there at this position? \"\"\"\n",
    "        return self.gridData[pos.y][pos.x]\n",
    "\n",
    "    def validPos(self, pos:Position):\n",
    "        \"\"\" is the position valid? we don't want to walk over walls or outside of the maze \"\"\"\n",
    "        return 0 <= pos.y < len(self.gridData) and 0 <= pos.x < len(self.gridData[pos.y]) and not self.isWall(pos)\n",
    "\n",
    "    def findFirst(self, key:int) -> Position:\n",
    "        y = 0\n",
    "        for row in self.gridData:\n",
    "            x = 0\n",
    "            for cell in row:\n",
    "                if cell == key:\n",
    "                    return Position(x, y)\n",
    "                x = x + 1\n",
    "            y = y + 1\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93223d22-4127-4929-a97b-3232135167c9",
   "metadata": {},
   "source": [
    "Next, the implementation for *State* and *Action* for the maze problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59040f8e-3698-421c-b76a-2c40b69b0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class GridAction(Action):\n",
    "    dir:Direction\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.dir)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.dir}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GridState(State):\n",
    "    pos:Position\n",
    "\n",
    "    def __init__(self, pos:Position):\n",
    "        self.pos = pos\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440d2ee-5d08-4142-9a5e-7e4382e1d192",
   "metadata": {},
   "source": [
    "The class ```GridAction``` extends the class ```Action``` that we defined before, adding the movement ```Direction```. The class ```GridState``` extends ```State```, and contains simply the ```Position``` of the agent.\n",
    "\n",
    "Finally we will implement the ```GridEnvironment```, the version of ```Environment``` tailored to our maze, as well as the definition of the Maze we want to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710b1e6f-ca11-4936-88ad-1ae082c152c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridEnvironment(Environment):\n",
    "    state:GridState\n",
    "    grid:Grid\n",
    "\n",
    "    def __init__(self, grid:Grid, startPosition:Position, endPosition:Position):\n",
    "        self.grid = grid\n",
    "        self.startPosition = startPosition\n",
    "        self.endPosition = endPosition\n",
    "        self.state = GridState(self.startPosition)\n",
    "\n",
    "    def isWinState(self):\n",
    "        return self.state.pos == self.endPosition\n",
    "\n",
    "    def isValidMove(self, dir:Direction):\n",
    "        next = self.state.pos.move(dir)\n",
    "        return self.grid.validPos(next) and not self.grid.isWall(next)\n",
    "\n",
    "    def getAllPossibleActions(self) -> List[Action]:\n",
    "        actions = []\n",
    "        for d in Direction:\n",
    "            if self.isValidMove(d):\n",
    "                actions.append(GridAction(d))\n",
    "        return actions\n",
    "\n",
    "    def getNewState(self, action:GridAction) -> GridState:\n",
    "        if not self.isValidMove(action.dir):\n",
    "            raise Exception(f\"Invalid action {action.dir} from {self.state.pos}\")\n",
    "\n",
    "        newPos = self.state.pos.move(action.dir)\n",
    "        return GridState(newPos)\n",
    "\n",
    "DEFAULT_MAZE = [[0,0,0,1,0,0,1,0],\n",
    "                [0,1,1,1,0,0,0,0],\n",
    "                [0,1,0,1,0,1,1,1],\n",
    "                [0,0,0,1,0,0,1,0],\n",
    "                [0,1,1,1,1,0,1,0],\n",
    "                [0,0,0,1,0,0,1,0],\n",
    "                [0,1,0,0,0,1,1,0],\n",
    "                [0,1,0,1,0,0,0,0],\n",
    "                ]\n",
    "\n",
    "DEFAULT_START = Position(0,0)\n",
    "DEFAULT_END = Position(5, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a846f-38b9-4169-af0a-2d5b4fc9860a",
   "metadata": {},
   "source": [
    "To better understand what each method of the class ```GridEnvironment``` does, go back to the implementation of ```Environment``` and see the differences between both.\n",
    "\n",
    "### How do we train?\n",
    "\n",
    "We have *almost* all pieces in place now, but we are still missing the code that trains the agent. This is implemented through two classes: ```Episode``` and ```Trainer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7648aa1e-9118-44d2-bdc6-0ab78ce6c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "NUM_TRAIN_EPISODES = 50\n",
    "NUM_TEST_EPISODES = 100\n",
    "NUM_BATCHES = 100\n",
    "MAX_STEPS = 50\n",
    "TEST_EXPLOIT_RATE = 0.75\n",
    "\n",
    "\n",
    "class Episode:\n",
    "    env: Environment\n",
    "    policy: Policy\n",
    "\n",
    "    def __init__(self, env:Environment, policy:Policy, exploitRate):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.exploitRate = exploitRate\n",
    "\n",
    "    def step(self, maxSteps = 100, onStepStart = lambda e:None, onStepEnd = lambda e:None):\n",
    "        steps = 0\n",
    "        while steps < maxSteps and not self.env.isEndState():\n",
    "            onStepStart(self.env)\n",
    "            action = self.policy.pickAction(self.env, self.exploitRate)\n",
    "            if not action:\n",
    "                break\n",
    "            oldState = self.env.getState()\n",
    "            reward = self.env.execute(action)\n",
    "            newState = self.env.getState()\n",
    "            self.policy.update(oldState, action, newState, reward)\n",
    "            onStepEnd(self.env)\n",
    "            steps = steps + 1\n",
    "\n",
    "        return steps\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    policy:Policy\n",
    "\n",
    "    def __init__(self, envProvider:Callable[[], Environment]):\n",
    "        \"\"\" envProvider is a function that returns an Environment for training and testing \"\"\"\n",
    "        self.envProvider = envProvider\n",
    "        self.rWins = []\n",
    "        self.eRates = []\n",
    "        self.nStates = []\n",
    "        self.nSteps = []\n",
    "        self.stateMaxRewards = []\n",
    "        self.policy = Policy(discountRate=1)\n",
    "\n",
    "    def trainExploitRate(self, batchNumber, numBatches):\n",
    "        return 0.5 + (batchNumber / numBatches)/2\n",
    "\n",
    "    def train(self, maxSteps, exploitRate):\n",
    "        env = self.envProvider()\n",
    "        episode = Episode(env, self.policy, exploitRate)\n",
    "        return episode.step(maxSteps)\n",
    "\n",
    "    def test(self, maxSteps, exploitRate):\n",
    "        env = self.envProvider()\n",
    "        episode = Episode(env, self.policy, exploitRate)\n",
    "        steps = episode.step(maxSteps)\n",
    "        return (env.isWinState(), steps, self.policy.getMaxReward(env.getState()))\n",
    "\n",
    "    def batchTrain(self, numBatches:int = NUM_BATCHES, numTrainEpisodes = NUM_TRAIN_EPISODES, numTestEpisodes = NUM_TEST_EPISODES, maxSteps = MAX_STEPS, maxProcesses:int = 1):\n",
    "        self.numBatches = numBatches\n",
    "\n",
    "        for b in tqdm(range(numBatches)):\n",
    "            exploitRate = self.trainExploitRate(b, numBatches)\n",
    "            if maxProcesses > 1:\n",
    "                with Pool(processes=maxProcesses) as pool:\n",
    "                    for _ in range(numTrainEpisodes):\n",
    "                        pool.apply_async(self.train, maxSteps, exploitRate)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "            else:\n",
    "                for _ in range(numTrainEpisodes):\n",
    "                    self.train(maxSteps, exploitRate)\n",
    "\n",
    "            wins = 0\n",
    "            totalSteps = 0\n",
    "            stateMaxReward = 0\n",
    "            for _ in range(numTestEpisodes):\n",
    "                (isWin, steps, maxReward) = self.test(maxSteps, TEST_EXPLOIT_RATE)\n",
    "                stateMaxReward = stateMaxReward + maxReward\n",
    "                totalSteps = totalSteps + steps\n",
    "                if isWin:\n",
    "                    wins = wins + 1\n",
    "\n",
    "            totalStates = self.policy.numKnownStates()\n",
    "            self.stateMaxRewards.append(stateMaxReward / numTestEpisodes)\n",
    "            self.rWins.append(wins)\n",
    "            self.eRates.append(exploitRate)\n",
    "            self.nStates.append(totalStates)\n",
    "            self.nSteps.append(totalSteps / numTestEpisodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175f53f-7c0e-4ff2-8e1f-5e034cdfd2e8",
   "metadata": {},
   "source": [
    "These classes might look more complex, but at their core they are really simple.\n",
    "\n",
    "The purpose of a ```Trainer``` is to train a ```Policy``` (remember that the objective of Q-Learning is to train a Policy function). The main entry point is the ```batchTrain``` method, which runs several *batches* of training, each containing multiple training *episodes*. We train in batches, because that allows us to review the progress of the training at regular intervals. The default is set to 100 batches, each one composed of 200 training episodes and 100 test episodes. This means that it will train 200 hundred times, then evaluate the current state of the policy with 100 episodes, and continue with the training, repeating this cycle 100 times / batches.\n",
    "\n",
    "Notice that the constructor for ```Trainer``` takes a function called ```envProvider``` as a parameter. The purpose of this function is to create a new suitable training environment every time we start a new *episode*. In this way, we *decouple* the trainer from the types of environments where we train, allowing us to use the same ```Trainer``` class for different types of environments. It also allows us to provide a new environment for every new training episode. In the case of a maze, this could mean starting from different positions in the maze. The ```trainExploitRate``` function will define the *Exploit Rate* to use for each episode based on which batch is being currently trained. The current implementation will start with an exploit rate of 50% (0.5) and increase it up to 100% (1). This means that, at the beginning, when the agent still knows very little of the maze, it will attempt to use its knowledge only half of the time. But the last training session, it will always attempt to exploit its knowledge.\n",
    "\n",
    "> #### Excercise: Can you think of reasons to make the exploit rate lower or higher?\n",
    "\n",
    "The ```Episode``` class implements the *episodes*. It simply asks the agent to act a certain number of *steps*, which is limited to prevent the training from never ending. The value to be used is entirely problem-dependant.\n",
    "\n",
    "With all that in mind, let's derfine a function that will create the training environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9ef91d-1d7e-4465-8a7f-98c6ff1ad6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def envBuilder(startPosition = DEFAULT_START, endPosition = DEFAULT_END):\n",
    "    return GridEnvironment(Grid(DEFAULT_MAZE), startPosition, endPosition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a2305-68f5-4d17-aad5-d2b8ae719f86",
   "metadata": {},
   "source": [
    "Finally we can create a trainer, and train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3114a5fb-1f07-468a-9c2b-d570edffa8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.53it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(envBuilder)\n",
    "trainer.batchTrain(numBatches = 50, numTrainEpisodes = 200, numTestEpisodes = 100, maxSteps = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fa153-b03f-4de9-a676-4f7c9107681f",
   "metadata": {},
   "source": [
    "> #### Exercise: What happens if we train fewer batches? How can we tell if we've trained the agent \"enough\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f262d79-4702-4ce9-90ca-fcdf0e4c36db",
   "metadata": {},
   "source": [
    "We have a trained agent (or rather, it's policy function), but how can we \"see\" it?\n",
    "\n",
    "One way is to take a look at the best actions for each state, and their expected accumulated reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df1008c-81c7-41fe-bdb0-00123c171a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Position(x=0, y=0), the best action is: Down, with expected reward of 82.0\n",
      "At Position(x=0, y=1), the best action is: Down, with expected reward of 83.0\n",
      "At Position(x=1, y=0), the best action is: Left, with expected reward of 81.0\n",
      "At Position(x=2, y=0), the best action is: Left, with expected reward of 80.0\n",
      "At Position(x=0, y=2), the best action is: Down, with expected reward of 84.0\n",
      "At Position(x=0, y=3), the best action is: Down, with expected reward of 85.0\n",
      "At Position(x=0, y=4), the best action is: Down, with expected reward of 86.0\n",
      "At Position(x=1, y=3), the best action is: Left, with expected reward of 84.0\n",
      "At Position(x=2, y=3), the best action is: Left, with expected reward of 83.0\n",
      "At Position(x=0, y=5), the best action is: Right, with expected reward of 87.0\n",
      "At Position(x=1, y=5), the best action is: Right, with expected reward of 88.0\n",
      "At Position(x=0, y=6), the best action is: Up, with expected reward of 86.0\n",
      "At Position(x=0, y=7), the best action is: Up, with expected reward of 85.0\n",
      "At Position(x=2, y=5), the best action is: Down, with expected reward of 89.0\n",
      "At Position(x=2, y=6), the best action is: Right, with expected reward of 90.0\n",
      "At Position(x=2, y=7), the best action is: Up, with expected reward of 89.0\n",
      "At Position(x=3, y=6), the best action is: Right, with expected reward of 91.0\n",
      "At Position(x=4, y=6), the best action is: Up, with expected reward of 92.0\n",
      "At Position(x=4, y=7), the best action is: Up, with expected reward of 91.0\n",
      "At Position(x=5, y=7), the best action is: Left, with expected reward of 90.0\n",
      "At Position(x=6, y=7), the best action is: Left, with expected reward of 89.0\n",
      "At Position(x=7, y=7), the best action is: Left, with expected reward of 88.0\n",
      "At Position(x=7, y=6), the best action is: Down, with expected reward of 86.99998710421015\n",
      "At Position(x=7, y=5), the best action is: Down, with expected reward of 85.52234210728032\n",
      "At Position(x=7, y=4), the best action is: Down, with expected reward of -45.68831724523063\n",
      "At Position(x=7, y=3), the best action is: Down, with expected reward of -39.18765993307507\n",
      "At Position(x=2, y=2), the best action is: Down, with expected reward of 82.0\n",
      "At Position(x=4, y=5), the best action is: Right, with expected reward of 93.0\n",
      "At Position(x=5, y=5), the best action is: Up, with expected reward of 94.0\n",
      "At Position(x=5, y=4), the best action is: Up, with expected reward of 95.0\n",
      "At Position(x=5, y=3), the best action is: Left, with expected reward of 96.0\n",
      "At Position(x=4, y=3), the best action is: Up, with expected reward of 97.0\n",
      "At Position(x=4, y=2), the best action is: Up, with expected reward of 98.0\n",
      "At Position(x=4, y=1), the best action is: Right, with expected reward of 99.0\n",
      "At Position(x=5, y=1), the best action is: Up, with expected reward of 100.0\n",
      "At Position(x=6, y=1), the best action is: Left, with expected reward of 99.0\n",
      "At Position(x=7, y=1), the best action is: Left, with expected reward of 98.0\n",
      "At Position(x=7, y=0), the best action is: Down, with expected reward of 97.0\n",
      "At Position(x=4, y=0), the best action is: Right, with expected reward of 100.0\n"
     ]
    }
   ],
   "source": [
    "def printPolicy(policy):\n",
    "    for (state, actionRewards) in policy.qMemory.sar.items():\n",
    "        bestAction = policy.qMemory.getBestAction(state)\n",
    "        if bestAction:\n",
    "            print (f\"At {state.pos}, the best action is: {bestAction.dir.name}, with expected reward of {policy.getMaxReward(state)}\")\n",
    "\n",
    "printPolicy(trainer.policy)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e45036-09c5-4e51-a539-df2ceb93b1fe",
   "metadata": {},
   "source": [
    "# Extra content - delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c602c6-54f4-4677-bf0c-c7ecdcd24232",
   "metadata": {},
   "source": [
    "Additionally, there are two important parameters:\n",
    "* $\\alpha$ is the \"learning rate\". As you can see in the formula, it influences how much the state will be modified by the new reward and the expected maximum reward at the new state. To make it easier to understand, imagine if $\\alpha$ is 0. As it multiplies the whole term at the right, it would go away and the Q value for $(S_t, A_t)$ would remain unchanged:\n",
    "$$\n",
    "Q(S_t, A_t) \\longleftarrow Q(S_t, A_t)\n",
    "$$\n",
    "\n",
    "Now imagine if the value of $\\alpha$ is 1:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\longleftarrow Q(S_t, A_t) + R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)\n",
    "$$\n",
    "The terms $Q(S_t, A_t)$ and $-Q(S_t, A_t)$ cancel out, leaving:\n",
    "$$\n",
    "Q(S_t, A_t) \\longleftarrow R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a)\n",
    "$$\n",
    "which means, the old value will be completely overriden with the new value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
